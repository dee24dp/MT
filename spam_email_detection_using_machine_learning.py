# -*- coding: utf-8 -*-
"""Spam_Email_Detection_using_Machine_Learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mIAJhJFIw9j8PX1hUy-K4s6v0254tf2o
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Load the dataset
data = pd.read_csv("/content/drive/MyDrive/dataset_mindshift/spam.csv", encoding='latin1')
data = data[['v1', 'v2']]  # Selecting relevant columns
data.columns = ['label', 'email']  # Renaming columns

import nltk
nltk.download('punkt_tab')
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

nltk.download('stopwords')
nltk.download('punkt')

def preprocess_text(text):
  """Preprocesses text by converting it to lowercase, removing punctuation, and stemming."""
  text = text.lower()
  text = re.sub(f"[{string.punctuation}]", "", text)
  tokens = word_tokenize(text)
  tokens = [word for word in tokens if word not in stopwords.words("english")]
  ps = PorterStemmer()
  tokens = [ps.stem(word) for word in tokens]
  return " ".join(tokens)


data = {"email": ["This is a test email.", "Another email with punctuation!"]}
data["processed_email"] = pd.Series(data["email"]).apply(preprocess_text)

print(data)

import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer

nltk.download('stopwords')
nltk.download('wordnet')

def preprocess_text(text):
  # Tokenize, remove stop words, lemmatize
  tokens = nltk.word_tokenize(text)
  stop_words = set(stopwords.words('english'))
  filtered_tokens = [w for w in tokens if w.lower() not in stop_words]
  lemmatizer = WordNetLemmatizer()
  lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]
  return ' '.join(lemmas)

data = {"email": ["This is a test email.", "Another email with punctuation!"]}
processed_emails = [preprocess_text(email) for email in data["email"]]
data["processed_email"] = processed_emails

# Now use TF-IDF
vectorizer = TfidfVectorizer()
tfidf_vectors = vectorizer.fit_transform(data["processed_email"])

print(tfidf_vectors)

from sklearn.model_selection import train_test_split

X = [1, 2, 3, 4]  # Assuming X contains your features
y = [1, 0, 1, 1]  # Assuming y contains your target labels

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Assuming 'data' is your DataFrame and contains the 'label' and 'processed_email' columns
X = data["processed_email"]
# Add a 'label' key to the data dictionary with example target labels
data['label'] = [0, 1]  # Example labels, replace with your actual labels
y = data["label"]

# Perform the train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Use TF-IDF to vectorize the text data
vectorizer = TfidfVectorizer()  # Initialize a TF-IDF vectorizer
X_train_vec = vectorizer.fit_transform(X_train)  # Fit and transform the training data
# Instead of reshaping, keep X_test as a list:
X_test_vec = vectorizer.transform(X_test)  # Transform the testing data

# Model training
model = MultinomialNB()
model.fit(X_train_vec, y_train)  # Use the TF-IDF vectors for training

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import numpy as np

# ... (your existing code to load and preprocess data)

# Assuming 'data' is your DataFrame and contains the 'label' and 'processed_email' columns
X = data["processed_email"]
# Add a 'label' key to the data dictionary with example target labels
data['label'] = [0, 1]  # Example labels, replace with your actual labels
y = data["label"]

# Perform the train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Use TF-IDF to vectorize the text data
vectorizer = TfidfVectorizer()  # Initialize a TF-IDF vectorizer
X_train_vec = vectorizer.fit_transform(X_train)  # Fit and transform the training data
# Instead of reshaping, keep X_test as a list:
X_test_vec = vectorizer.transform(X_test)  # Transform the testing data

# Model training
model = MultinomialNB()
model.fit(X_train_vec, y_train)  # Use the TF-IDF vectors for training

# Predictions
y_pred = model.predict(X_test_vec)

# Evaluation metrics
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')
nltk.download('wordnet')


def preprocess_text(text):
    # Tokenize, remove stop words, lemmatize
    tokens = nltk.word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [w for w in tokens if w.lower() not in stop_words]
    lemmatizer = WordNetLemmatizer()
    lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]
    return ' '.join(lemmas)


def predict_email(email, model, vectorizer):
    """Predicts whether an email is spam (1) or ham (0).

    Args:
        email (str): The email text.
        model: The trained classification model.
        vectorizer: The TF-IDF vectorizer.

    Returns:
        int: 0 for ham, 1 for spam.
    """
    processed_email = preprocess_text(email)
    email_vec = vectorizer.transform([processed_email])  # Transform using the fitted vectorizer
    prediction = model.predict(email_vec)
    return prediction[0]


# Example usage:
data = {"email": ["This is a test email.", "Another email with punctuation!"]}
processed_emails = [preprocess_text(email) for email in data["email"]]
data["processed_email"] = processed_emails
data['label'] = [0, 1]  # Example labels, replace with your actual labels

X = data["processed_email"]
y = data["label"]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

vectorizer = TfidfVectorizer()
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, y_train)

# Now you can predict new emails:
new_email = "This is a new email I want to classify."
prediction = predict_email(new_email, model, vectorizer)

if prediction == 0:
    print("The email is classified as ham.")
else:
    print("The email is classified as spam.")